{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os \n",
    "import urllib\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import GPT2Model\n",
    "from gepeto.model import MODEL_ARCHITECTURES, GPTModel\n",
    "from gepeto.utils import load_weights\n",
    "from gepeto.train import calc_loss_loader, train_model_simple\n",
    "from gepeto.token import token_ids_to_text, text_to_token_ids\n",
    "from gepeto.generate import generate\n",
    "from gepeto.loader import InstructionDataset, format_input, custom_collate_fn\n",
    "\n",
    "import tiktoken\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "        \n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "    \n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    return data\n",
    "\n",
    "file_path = \"../data/instruction-data.json\"\n",
    "\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = format_input(data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set lenght: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion: train_portion+test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(\"Training set lenght:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device='cpu'\n",
    "):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device='cpu'\n",
    "):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = custom_collate_draft_2(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,     3,     4],\n",
       "        [    5,     6, 50256, 50256, 50256],\n",
       "        [    7,     8,     9, 50256, 50256]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     2,     3,     4, 50256],\n",
       "        [    6, 50256, 50256, 50256, 50256],\n",
       "        [    8,     9, 50256, 50256, 50256]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = custom_collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "    [-0.5, 1.5]\n",
    "    ]\n",
    ")\n",
    "\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor(\n",
    "    [\n",
    "        [-1.0, 1.0],\n",
    "        [-0.5, 1.5],\n",
    "        [-0.5, 1.5],\n",
    "    ]\n",
    ")\n",
    "\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "logits_3 = torch.tensor(\n",
    "    [\n",
    "        [-1.0, 1.0],\n",
    "        [-0.5, 1.5],\n",
    "        [-0.5, 1.5],\n",
    "    ]\n",
    ")\n",
    "\n",
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_workes = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workes\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workes\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6a8105631f4eaf960c217003e19bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5241fb6e5ba143a8aaec5109b3399ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt_pretrained = GPT2Model.from_pretrained(\"openai-community/gpt2-medium\", cache_dir=\"checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_configs = MODEL_ARCHITECTURES['gpt2-medium']\n",
    "\n",
    "custom_configs = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"drop_rate\": 0.0,  # Dropout rate\n",
    "    \"qkv_bias\": True,  # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "model_configs = base_configs | custom_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTModel(model_configs) # Create an empty model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (transformer_decoders): Sequential(\n",
       "    (0): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerDecoder(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_layer1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_layer2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_foward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTModel(model_configs) # Create an empty model\n",
    "load_weights(gpt, gpt_pretrained, model_configs) # Transfer the parameters from HuggingFace's model to our empty model\n",
    "gpt.to(device) # Send the computation to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "    max_new_tokens=35,\n",
    "    context_size=model_configs[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = generated_text[len(input_text):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active\n"
     ]
    }
   ],
   "source": [
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(\n",
    "        train_loader, gpt, device, num_batches=5\n",
    "    )\n",
    "    val_loss = calc_loss_loader(\n",
    "        val_loader, gpt, device, num_batches=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.01228551864624\n",
      "Validation loss: 3.9374598979949953\n"
     ]
    }
   ],
   "source": [
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 000000): Train loss 2.774, Val loss 2.753\n",
      "Epoch 1 (Step 000005): Train loss 1.206, Val loss 1.137\n",
      "Epoch 1 (Step 000010): Train loss 0.871, Val loss 0.968\n",
      "Epoch 1 (Step 000015): Train loss 0.856, Val loss 0.906\n",
      "Epoch 1 (Step 000020): Train loss 0.788, Val loss 0.910\n",
      "Epoch 1 (Step 000025): Train loss 0.774, Val loss 0.865\n",
      "Epoch 1 (Step 000030): Train loss 0.801, Val loss 0.841\n",
      "Epoch 1 (Step 000035): Train loss 0.716, Val loss 0.812\n",
      "Epoch 1 (Step 000040): Train loss 0.669, Val loss 0.801\n",
      "Epoch 1 (Step 000045): Train loss 0.633, Val loss 0.790\n",
      "Epoch 1 (Step 000050): Train loss 0.663, Val loss 0.784\n",
      "Epoch 1 (Step 000055): Train loss 0.763, Val loss 0.771\n",
      "Epoch 1 (Step 000060): Train loss 0.720, Val loss 0.748\n",
      "Epoch 1 (Step 000065): Train loss 0.651, Val loss 0.739\n",
      "Epoch 1 (Step 000070): Train loss 0.530, Val loss 0.732\n",
      "Epoch 1 (Step 000075): Train loss 0.566, Val loss 0.731\n",
      "Epoch 1 (Step 000080): Train loss 0.604, Val loss 0.724\n",
      "Epoch 1 (Step 000085): Train loss 0.511, Val loss 0.706\n",
      "Epoch 1 (Step 000090): Train loss 0.565, Val loss 0.693\n",
      "Epoch 1 (Step 000095): Train loss 0.501, Val loss 0.687\n",
      "Epoch 1 (Step 000100): Train loss 0.501, Val loss 0.680\n",
      "Epoch 1 (Step 000105): Train loss 0.567, Val loss 0.674\n",
      "Epoch 1 (Step 000110): Train loss 0.555, Val loss 0.667\n",
      "Epoch 1 (Step 000115): Train loss 0.509, Val loss 0.666\n",
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task.Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
      "Epoch 2 (Step 000120): Train loss 0.435, Val loss 0.671\n",
      "Epoch 2 (Step 000125): Train loss 0.448, Val loss 0.685\n",
      "Epoch 2 (Step 000130): Train loss 0.448, Val loss 0.680\n",
      "Epoch 2 (Step 000135): Train loss 0.409, Val loss 0.679\n",
      "Epoch 2 (Step 000140): Train loss 0.411, Val loss 0.679\n",
      "Epoch 2 (Step 000145): Train loss 0.369, Val loss 0.682\n",
      "Epoch 2 (Step 000150): Train loss 0.378, Val loss 0.675\n",
      "Epoch 2 (Step 000155): Train loss 0.412, Val loss 0.675\n",
      "Epoch 2 (Step 000160): Train loss 0.411, Val loss 0.682\n",
      "Epoch 2 (Step 000165): Train loss 0.377, Val loss 0.688\n",
      "Epoch 2 (Step 000170): Train loss 0.324, Val loss 0.686\n",
      "Epoch 2 (Step 000175): Train loss 0.337, Val loss 0.670\n",
      "Epoch 2 (Step 000180): Train loss 0.392, Val loss 0.656\n",
      "Epoch 2 (Step 000185): Train loss 0.419, Val loss 0.658\n",
      "Epoch 2 (Step 000190): Train loss 0.340, Val loss 0.648\n",
      "Epoch 2 (Step 000195): Train loss 0.329, Val loss 0.632\n",
      "Epoch 2 (Step 000200): Train loss 0.307, Val loss 0.629\n",
      "Epoch 2 (Step 000205): Train loss 0.349, Val loss 0.623\n",
      "Epoch 2 (Step 000210): Train loss 0.371, Val loss 0.624\n",
      "Epoch 2 (Step 000215): Train loss 0.397, Val loss 0.632\n",
      "Epoch 2 (Step 000220): Train loss 0.302, Val loss 0.646\n",
      "Epoch 2 (Step 000225): Train loss 0.338, Val loss 0.659\n",
      "Epoch 2 (Step 000230): Train loss 0.292, Val loss 0.656\n",
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task.Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
      "Training completed in 8.56 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    gpt,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=format_input(val_data[0]),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f'Training completed in {execution_time_minutes:.2f} minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepeto.plot import plot_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7ddc1a1530>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "---------------------------------\n",
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "---------------------------------\n",
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in test_data[:3]:\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate(\n",
    "        model=gpt,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=model_configs['context_length'],\n",
    "        eos_id=50256\n",
    "    )\n",
    "\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 110/110 [00:43<00:00,  2.53it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate(\n",
    "        model=gpt,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=model_configs[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "with open(\"../data/instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a bullet.'}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "model_filepath = \"../data/gpt2-medium-sft.pth\"\n",
    "torch.save(gpt.state_dict(), model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter(['name']):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\n",
    "        \"ollama not running. Launch ollama before preecending.\"\n",
    "    )\n",
    "\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "\n",
    "def query_model(\n",
    "    prompt,\n",
    "    model=\"llama3\",\n",
    "    url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "        return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories and low in fiber.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach. These treats are great for providing essential vitamins and minerals.\n",
      "5. Minerals: Llamas need access to mineral supplements, which provide essential nutrients like calcium, phosphorus, and salt.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "* Leaves from trees and shrubs\n",
      "* Bark (in some cases)\n",
      "* Mosses and lichens\n",
      "\n",
      "Domesticated llamas typically receive a balanced diet that includes a mix of these food sources. Their owners or caretakers may also provide additional nutrients, such as vitamins and minerals, to ensure the llama's overall health and well-being.\n",
      "\n",
      "Remember, every llama is different, and their dietary needs can vary depending on factors like age, size, and activity level. If you're considering owning a llama, it's essential to consult with an experienced breeder or veterinarian to determine the best diet for your new furry friend!\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/instruction-data-with-response.json\", \"r\") as file:\n",
    "    json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "\n",
      "Score:\n",
      ">> I'd rate the model response \"The car is as fast as a bullet.\" an 85 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The response uses a simile correctly, comparing the speed of the car to something else (in this case, a bullet).\n",
      "* The comparison is reasonable and easy to understand. Bullets are known for their high velocity, making it a good analogy for a fast-moving car.\n",
      "* The phrase \"as fast as\" is used correctly to introduce the simile.\n",
      "\n",
      "The only reason I wouldn't give it a perfect score is that some people might not immediately think of bullets when they hear \"fast,\" and the comparison might not be as universally relatable as lightning (which is often associated with speed). However, overall, the response is well-crafted and effectively conveys the idea that the car is very fast.\n",
      "\n",
      "---------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "\n",
      "Score:\n",
      ">> I'd score this model response as 40 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The model correctly identifies that thunderstorms are related to clouds (correctly identifying the type of phenomenon).\n",
      "* However, it incorrectly specifies the type of cloud associated with thunderstorms. Cumulus clouds are not typically associated with thunderstorms; cumulonimbus clouds are.\n",
      "* The response lacks precision and accuracy in its description.\n",
      "\n",
      "Overall, while the model attempts to address the question, it provides an incorrect answer, which is why I'd score it as 40 out of 100.\n",
      "\n",
      "---------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Score:\n",
      ">> I'd rate my own response as 95 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The response accurately answers the question by naming the author of 'Pride and Prejudice' as Jane Austen.\n",
      "* The response is concise and to the point, providing a clear and direct answer.\n",
      "* There are no grammatical errors or ambiguities in the response.\n",
      "\n",
      "The only reason I wouldn't give myself a perfect score is that the response is slightly redundant - it's not adding any new information or insights beyond simply stating the correct answer. However, for a simple question like this one, a clear and accurate response is exactly what's needed!\n",
      "\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in json_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_scores(json_data, json_key, model='llama3'):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc='Scoring entries'):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|| 110/110 [00:08<00:00, 13.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 88.64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_from_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
